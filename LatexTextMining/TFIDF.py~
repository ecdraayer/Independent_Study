import nltk
import os, sys, re
import string

from collections import Counter
from nltk.corpus import stopwords
from nltk.stem.porter import *
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer


def stem_tokens(tokens, stemmer):
  stemmed = []
  for item in tokens:
    stemmed.append(stemmer.stem(item))
  return stemmed
  
def tokenize(text):
  tokens = nltk.word_tokenize(text)
  stems = stem_tokens(tokens, stemmer)
  return stems  



token_dict = {}
stemmer = PorterStemmer()

for filename in os.listdir(sys.argv[1]):
  ResearchPaper = open(os.path.join(sys.argv[1], filename), 'r')
  text = ResearchPaper.read()
  lowers = text.lower()
  no_punctuation = lowers.translate(None, string.punctuation)
  token_dict[file] = no_punctuation
    

tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')
tfs = tfidf.fit_transform(token_dict.values())
feature_names = tfidf.get_feature_names()
for col in tfs.nonzero()[1]:
  print feature_names[col], ' - ', tfs[0, col]
